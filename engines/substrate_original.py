#!/usr/bin/env python3
"""
================================================================================
HILBERT SUBSTRATE - GPU/CPU BACKEND
================================================================================

Substrate engine for the Resonance Lab tooling.

Public API
----------
    from substrate import Config, Substrate, run_simulation

Objects
-------
- Config
    Lightweight dataclass collecting model parameters.

- Substrate
    Owns the graph, node Hilbert states, and evolution rules.

- SubstrateNode
    Thin wrapper used by analysis code, giving per-node helpers:
        node.state
        node.direction_amplitudes()
        node.total_entanglement
        node.n_connections
        node.neighbor_ids
        node.neighbors

Design constraints
------------------
We stay inside your three axioms:

  1. Hilbert-space realism:
       The fundamental object is a big Hilbert space with local DOFs.
  2. Unitary evolution:
       Time evolution is generated by a Hermitian coupling matrix J.
  3. Classical emergence:
       Graph structure & "defrag" encode how effective space / locality
       emerge from entanglement patterns.

Defrag is an *information-based* operation:
  - "Information that belongs together" = large, coherent coupling weight.
  - "Put together"   = concentrate that weight on a few nearby neighbors.
  - "Force spacetime" = prefer strong couplings between nodes that are
                        already close in the emergent graph distance.

Crucially for Axiom 2:
  - Once a coupling J_ij is nonzero, defrag never sets it back to *exact* 0.
    It can make it arbitrarily small, but topological adjacency is fixed
    after initialization.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

# Try to get CuPy for GPU. Fall back gracefully to NumPy.
try:
    import cupy as cp  # type: ignore
    _HAS_CUPY = True
except Exception:  # GPU is optional
    cp = None      # type: ignore
    _HAS_CUPY = False


# =============================================================================
# Config
# =============================================================================

@dataclass
class Config:
    """
    Minimal configuration for the substrate.

    Parameters
    ----------
    n_nodes : int
        Number of graph nodes.
    internal_dim : int
        Hilbert-space dimension per node.
    monogamy_budget : float
        Per-node "entanglement budget" used to scale couplings.
    defrag_rate : float
        Step size for the information-defrag update. Typical O(0.1–0.5).
    dt : float
        Time step for the simple first-order evolution integrator.
    seed : int
        Random seed for reproducibility.
    connectivity : float
        Approximate probability of an edge between node pairs at init.
    use_gpu : Optional[bool]
        None  -> auto-detect CuPy, prefer GPU if available.
        True  -> force GPU (warning if unavailable).
        False -> force CPU (NumPy).

    defrag_cutoff : float
        Kept for backwards compatibility with older scripts but *not used*
        to zero-out couplings in the dynamics anymore. Set it to 0.0 or
        ignore it entirely.

    pressure_p, pressure_q, pressure_lambda, pressure_gamma
        Kept for backwards compatibility with earlier drivers; not used
        in the current defrag implementation, but harmless.
    """

    n_nodes: int = 32
    internal_dim: int = 3
    monogamy_budget: float = 1.0
    defrag_rate: float = 0.1
    dt: float = 0.1
    seed: int = 42
    connectivity: float = 0.35
    use_gpu: Optional[bool] = None  # None = auto-detect CuPy

    defrag_cutoff: float = 0.0  # not used in dynamics (no edge deletion)

    # Unused in current defrag, but kept so existing scripts still work
    pressure_p: float = 0.5
    pressure_q: float = 2.0
    pressure_lambda: float = 0.0
    pressure_gamma: float = 1.0


# =============================================================================
# Backend helpers
# =============================================================================

def _select_backend(use_gpu: Optional[bool]) -> Tuple[Any, str]:
    """
    Return (xp, backend_name) given the user's preference and availability.
    xp is either numpy or cupy.
    """
    if use_gpu is True and not _HAS_CUPY:
        print("[substrate] WARNING: use_gpu=True but CuPy not available; using NumPy CPU.")
    if use_gpu is False or not _HAS_CUPY:
        return np, "NumPy CPU"
    # use_gpu is None or True and CuPy is available
    return cp, "CuPy GPU"  # type: ignore[return-value]


def _to_numpy(x) -> np.ndarray:
    """Convert xp array (numpy or cupy) to NumPy ndarray."""
    if hasattr(x, "get"):  # CuPy ndarray
        return x.get()
    return np.asarray(x)


# =============================================================================
# Node wrapper
# =============================================================================

class SubstrateNode:
    """
    Lightweight node wrapper exposing the API expected by the engines.

        node.state                : complex vector (xp array)
        node.direction_amplitudes(): NumPy view of state
        node.total_entanglement   : scalar (sum_j |J_ij|)
        node.n_connections        : degree
        node.neighbor_ids         : list[int]
        node.neighbors            : list[SubstrateNode]
    """

    def __init__(self, substrate: "Substrate", idx: int):
        self._substrate = substrate
        self._idx = idx
        # Human-readable ID
        self.id = f"n{idx:03d}"

    # --------------------- core state ---------------------

    @property
    def state(self):
        return self._substrate.states[self._idx]

    @state.setter
    def state(self, value):
        xp = self._substrate.xp
        arr = value
        if not isinstance(arr, xp.ndarray):
            arr = xp.asarray(arr, dtype=xp.complex128)
        self._substrate.states[self._idx] = arr

    def direction_amplitudes(self) -> np.ndarray:
        """Return the node's local Hilbert state as a NumPy array."""
        return _to_numpy(self.state)

    # -------------------- graph info ----------------------

    @property
    def neighbor_ids(self) -> List[int]:
        """Return integer indices of neighbors."""
        return list(self._substrate._neighbors[self._idx])

    @property
    def neighbors(self) -> List["SubstrateNode"]:
        """Return SubstrateNode wrappers for each neighbor."""
        return [self._substrate._nodes[j] for j in self._substrate._neighbors[self._idx]]

    @property
    def n_connections(self) -> int:
        """Graph degree."""
        return len(self._substrate._neighbors[self._idx])

    # ---------------- entanglement proxy ------------------

    @property
    def total_entanglement(self) -> float:
        """
        Crude per-node "entanglement" proxy:
            sum_j |J_ij|
        """
        J = self._substrate.couplings
        xp = self._substrate.xp
        row = J[self._idx]
        return float(_to_numpy(xp.sum(xp.abs(row))))


# =============================================================================
# Substrate core
# =============================================================================

class Substrate:
    """
    Hilbert substrate with a graph of nodes, each carrying a local Hilbert state.

    Attributes
    ----------
    n_nodes : int
        Number of nodes in the graph.
    d : int
        Local Hilbert-space dimension (internal_dim).
    states : xp.ndarray, shape (n_nodes, d)
        Local state vectors for each node.
    couplings : xp.ndarray, shape (n_nodes, n_nodes)
        Hermitian coupling matrix J_ij between nodes.
    _neighbors : List[List[int]]
        Adjacency list derived from non-zero couplings.
    xp : module
        Either numpy or cupy, depending on backend selection.
    """

    # ----------------------- init -------------------------

    def __init__(self, config: Config):
        self.config = config
        self.n_nodes = int(config.n_nodes)
        self.d = int(config.internal_dim)
        self.monogamy_budget = float(config.monogamy_budget)
        self.defrag_rate = float(config.defrag_rate)
        self.dt = float(config.dt)
        self.connectivity = float(config.connectivity)

        # RNG for graph / couplings / states (NumPy-based for reproducibility)
        self._rng = np.random.default_rng(config.seed)

        # Backend selection
        xp, backend_name = _select_backend(config.use_gpu)
        self.xp = xp  # uses property setter below

        # Allocate state and coupling arrays
        self.states = self.xp.zeros((self.n_nodes, self.d), dtype=self.xp.complex128)
        self.couplings = self.xp.zeros((self.n_nodes, self.n_nodes), dtype=self.xp.complex128)

        # Neighbor list (Python list-of-lists, used by pattern_detector & friends)
        self._neighbors: List[List[int]] = [[] for _ in range(self.n_nodes)]

        # Initialize graph / couplings / states
        self._init_random_graph_and_couplings()
        self._init_random_states()

        # Build SubstrateNode wrappers
        self._nodes: Dict[int, SubstrateNode] = {
            i: SubstrateNode(self, i) for i in range(self.n_nodes)
        }

        print(f"[substrate] Using {backend_name} backend (batched).")
        print(
            f"[substrate] Batched substrate: {self.n_nodes} nodes, "
            f"{self.num_edges} edges"
        )

    # ------------------- basic properties -------------------

    @property
    def xp(self):
        """Backend module: numpy or cupy."""
        return self._xp

    @xp.setter
    def xp(self, value):
        self._xp = value

    @property
    def num_edges(self) -> int:
        """Each undirected edge counted once."""
        xp = self.xp
        J = self.couplings
        if xp is np:
            return int(np.count_nonzero(np.abs(J) > 0) // 2)
        else:  # CuPy
            return int(int((xp.abs(J) > 0).sum()) // 2)

    @property
    def neighbors(self) -> Dict[int, List[int]]:
        """
        Public view of the neighbor structure, as expected by pattern_detector:

            {node_index: [neighbor_indices]}
        """
        return {i: list(nbrs) for i, nbrs in enumerate(self._neighbors)}

    @property
    def nodes(self) -> Dict[int, SubstrateNode]:
        """Mapping from integer node index to SubstrateNode wrapper."""
        return self._nodes

    # ------------------- initialization ---------------------

    def _init_random_states(self) -> None:
        """Randomly initialize local states as normalized complex vectors."""
        xp = self.xp
        n, d = self.n_nodes, self.d

        # Use NumPy RNG, then transfer to xp
        real = self._rng.normal(loc=0.0, scale=1.0, size=(n, d))
        imag = self._rng.normal(loc=0.0, scale=1.0, size=(n, d))
        psi_np = real + 1j * imag

        # Normalize each row
        norms = np.linalg.norm(psi_np, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        psi_np /= norms

        self.states = xp.asarray(psi_np, dtype=xp.complex128)

    def _init_random_graph_and_couplings(self) -> None:
        """
        Initialize a random undirected graph and a Hermitian coupling matrix J.

        For each pair i<j, we create an edge with probability `connectivity`
        and assign a random complex weight. Then we symmetrize J and normalize
        each row so that sum_j |J_ij| ≈ monogamy_budget.
        """
        xp = self.xp
        n = self.n_nodes
        p = self.connectivity

        # Build adjacency + couplings in NumPy first
        J_real = self._rng.normal(loc=0.0, scale=1.0, size=(n, n))
        J_imag = self._rng.normal(loc=0.0, scale=1.0, size=(n, n))
        J_np = J_real + 1j * J_imag

        # Sparse-ish via Bernoulli mask
        mask = self._rng.random((n, n)) < p
        J_np *= mask

        # Remove self-couplings
        np.fill_diagonal(J_np, 0.0)

        # Symmetrize: undirected graph, Hermitian J
        J_np = 0.5 * (J_np + J_np.T.conj())

        # Normalize each row to match monogamy_budget
        row_abs = np.abs(J_np)
        row_sums = row_abs.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0.0] = 1.0
        J_np *= (self.monogamy_budget / row_sums)

        # Convert to backend array
        self.couplings = xp.asarray(J_np, dtype=xp.complex128)

        # Build neighbor list
        self._rebuild_neighbors_from_couplings()

    def _rebuild_neighbors_from_couplings(self) -> None:
        """
        Refresh self._neighbors from self.couplings.

        NOTE: Because we no longer hard-delete couplings once nonzero,
        the adjacency is effectively fixed after initialization. Entries that
        were zero at t=0 stay zero; entries that were nonzero remain >0
        (at least at machine precision).
        """
        J_np = _to_numpy(self.couplings)
        n = J_np.shape[0]
        self._neighbors = []
        for i in range(n):
            nbrs = np.nonzero(np.abs(J_np[i]) > 0.0)[0].tolist()
            self._neighbors.append(nbrs)

    # ------------------- evolution --------------------------

    def _hamiltonian_action(self) -> Any:
        """
        Compute H |psi> as a coupling of node amplitudes:

            (H psi)_i = sum_j J_ij * psi_j

        We don't build H explicitly; we just compute J @ psi.
        """
        xp = self.xp
        J = self.couplings
        psi = self.states  # shape (N, d)
        return xp.matmul(J, psi)

    def evolve(self, n_steps: int = 1, defrag_rate: Optional[float] = None) -> None:
        """
        Advance the substrate by `n_steps` of simple unitary + defrag.

        Evolution rule (very simple first-order integrator):

            |psi> <- |psi> + (-i dt) * H |psi>
            (then per-node normalization to keep things well-behaved)

        After each Hamiltonian step we apply a single defrag_step, which
        is a local, information-based update of one row of J.

        Defrag respects Axiom 2:
          - once J_ij is nonzero, it is never set back to exact 0 by defrag.
          - magnitudes can become arbitrarily small, but not vanish.
        """
        xp = self.xp
        if defrag_rate is None:
            defrag_rate = self.defrag_rate
        defrag_rate = float(defrag_rate)

        for _ in range(int(n_steps)):
            # Hamiltonian action
            Hpsi = self._hamiltonian_action()
            self.states = self.states + (-1j * self.dt) * Hpsi

            # Normalize each node's local state
            norms = xp.linalg.norm(self.states, axis=1, keepdims=True)
            norms = xp.where(norms > 0, norms, 1.0)
            self.states = self.states / norms

            # Information-based defragmentation of couplings
            if defrag_rate > 0.0:
                self.defrag_step(rate=defrag_rate)

    # ------------------- graph distance ---------------------

    def _graph_distances_from(self, src: int) -> np.ndarray:
        """
        BFS distances from src to all nodes in the current topological graph.

        Returns
        -------
        dist : np.ndarray[float], shape (n_nodes,)
            dist[j] = shortest path length from src to j, or np.inf if unreachable.
        """
        from collections import deque

        n = self.n_nodes
        neighbors = self._neighbors

        src = int(src)
        if src < 0 or src >= n:
            raise ValueError(f"src index {src} out of range 0..{n-1}")

        dist = np.full(n, np.inf, dtype=float)
        dist[src] = 0.0

        q = deque([src])
        while q:
            i = q.popleft()
            di = dist[i]
            for j in neighbors[i]:
                if not np.isfinite(dist[j]):
                    dist[j] = di + 1.0
                    q.append(j)

        return dist

    def graph_distance(self, src_idx: int, tgt_idx: int, max_radius: int = 10) -> float:
        """
        Breadth-first search distance between two nodes in the *topological*
        graph defined by _neighbors (ignores coupling magnitudes).

        Returns
        -------
        dist : float
            Integer distance in edges, or np.inf if beyond max_radius.
        """
        from collections import deque

        src_idx = int(src_idx)
        tgt_idx = int(tgt_idx)

        if src_idx == tgt_idx:
            return 0.0

        q = deque()
        q.append((src_idx, 0))
        visited = {src_idx}

        while q:
            i, d = q.popleft()
            if d >= max_radius:
                continue
            for j in self._neighbors[i]:
                if j == tgt_idx:
                    return float(d + 1)
                if j not in visited:
                    visited.add(j)
                    q.append((j, d + 1))

        return float(np.inf)

    # ------------------- defrag (information-based) --------

    def defrag_step(self, rate: float = 0.1) -> None:
        """
        Information-based defrag step on one randomly chosen row.

        Conceptual rule:
          - Let w_ij = |J_ij| be the local "information links" from node i.
          - Let p_ij = w_ij / sum_j w_ij be a probability distribution over neighbors.
          - Let d_ij be the current graph distance from i to j.

        We want:
          - information to concentrate on a few important destinations (low entropy),
          - strong couplings to align with short distances in the emergent graph.

        We approximate a gradient step on a cost functional that combines
        "spread" and "distance-penalized weight" by:

            weights_new ∝ p_ij^(1 + rate) * exp( - rate * d_ij^2 )

        That is:
          - p^(1+rate)  : sharpens high-probability neighbors ("belongs together").
          - exp(-rate d^2): suppresses strong long-distance links.

        Then we:
          - renormalize to the node's monogamy_budget,
          - enforce "once nonzero, always nonzero" at machine precision,
          - restore Hermitian symmetry and rebuild neighbors.

        NOTE: We explicitly *do not* delete existing couplings:
          any entry that was nonzero before remains at least tiny > 0 after.
        """
        xp = self.xp
        J = self.couplings
        n = self.n_nodes

        if n < 2:
            return

        # Choose random row i to update
        i = int(self._rng.integers(0, n))
        row = J[i].copy()

        # Magnitudes and phases
        mag = xp.abs(row)
        total_mag = float(_to_numpy(xp.sum(mag)))
        if total_mag <= 0.0:
            return

        phases = xp.exp(1j * xp.angle(row))

        # Old magnitudes (NumPy) for "once nonzero, always nonzero"
        old_mag_np = _to_numpy(mag)  # shape (n,)

        # Work in NumPy for the information + distance calculus
        mag_np = old_mag_np.copy()

        # Normalize magnitudes -> local probabilities p_ij
        p_np = mag_np / total_mag

        # Graph distances from i
        dist_np = self._graph_distances_from(i)  # shape (n,)
        dist2_np = dist_np ** 2

        # For unreachable nodes (inf), exp(-rate * inf) -> 0 by hand
        far_mask = ~np.isfinite(dist2_np)
        dist2_np[far_mask] = np.inf

        # Sharpen probabilities and bias toward short distances.
        # Small epsilon to avoid log(0) / 0**something issues.
        eps = 1e-15
        p_safe = np.maximum(p_np, eps)

        alpha = 1.0 + rate    # entropy-sharpening exponent
        # entropy / cohesion part
        p_sharp = p_safe ** alpha

        # distance penalty part
        penalty = np.exp(-rate * dist2_np)
        penalty[far_mask] = 0.0

        weights = p_sharp * penalty

        # If everything got killed (e.g., all far), fall back to p_sharp
        if not np.any(weights > 0.0):
            weights = p_sharp

        # Renormalize weights to sum 1
        w_sum = float(weights.sum())
        if w_sum <= 0.0:
            return
        weights /= w_sum

        # Scale to monogamy budget: sum_j |J_ij| = monogamy_budget
        mag_new_np = weights * self.monogamy_budget

        # Enforce "once nonzero, always nonzero" at machine precision:
        tiny = 1e-15
        mask_was_nonzero = old_mag_np > 0.0
        mag_new_np[mask_was_nonzero] = np.maximum(
            mag_new_np[mask_was_nonzero],
            tiny,
        )

        # Back to xp
        mag_new = xp.asarray(mag_new_np, dtype=xp.float64)

        # Recompose row with new magnitudes, same phases
        row_new = phases * mag_new

        # No self-coupling
        row_new[i] = 0.0

        # Write back row i and enforce Hermitian symmetry
        J[i] = row_new
        for j in range(n):
            if j == i:
                continue
            J[j, i] = xp.conj(J[i, j])

        # Rebuild neighbor list after topology changes
        # (Adjacency is effectively frozen for entries that were ever nonzero.)
        self._rebuild_neighbors_from_couplings()

    # ------------------- diagnostics ------------------------

    def total_entanglement_entropy(self) -> float:
        """
        Very crude "global entanglement" proxy:

            S_tot = sum_i sum_j |J_ij|

        (This is *not* a von Neumann entropy; it's just a scalar diagnostic
        for how much coupling weight exists in the graph.)
        """
        xp = self.xp
        J = self.couplings
        row_abs = xp.abs(J)
        s = float(_to_numpy(xp.sum(row_abs)))
        return s


# =============================================================================
# run_simulation helper
# =============================================================================

from typing import Dict as _DictAlias  # avoid shadowing above types


def run_simulation(
    config: Config,
    n_steps: int,
    record_every: int = 1,
) -> Tuple[Substrate, _DictAlias[str, Any]]:
    """
    Convenience function used by various probe scripts.

    Parameters
    ----------
    config : Config
    n_steps : number of evolution steps
    record_every : record substrate snapshots every this many steps

    Returns
    -------
    substrate : final Substrate object
    history : dict with lightweight diagnostics
              currently just stores a few timepoints of global entanglement.
    """
    sub = Substrate(config)
    history_ent: List[float] = []
    times: List[float] = []

    for step in range(n_steps + 1):
        if step % record_every == 0:
            history_ent.append(sub.total_entanglement_entropy())
            times.append(step * config.dt)
        if step < n_steps:
            sub.evolve(n_steps=1)

    history: Dict[str, Any] = {
        "times": np.asarray(times, dtype=float),
        "total_entanglement": np.asarray(history_ent, dtype=float),
    }
    return sub, history
