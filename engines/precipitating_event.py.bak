"""
precipitating_event.py

Single-file Hilbert-substrate experiment with:

  1) Optional gauge-field self-organization on an emergent geometry.
  2) A Precipitating Event (quench) using that geometry and gauge background.

Philosophy:
- Only Hilbert space + constraints + emergent geometry.
- No ad hoc particle fields or hand-picked particle initial conditions.
- Gauge field = phases on links that track how information transforms
  as it moves through geometry (parallel transport).
- Precipitating Event = quench/cooling where localized "lumps" in <Z_i>
  emerge as proto-particles.

High-level flow:
- Load geometry (graph_dist).
- Build adjacency & edges.
- (Optional) Run gauge optimization:
    * choose link phases to:
        - penalize long-range correlations
        - reward nearest-neighbor correlations
        - prefer ~π flux on square plaquettes
- Run Precipitating Event:
    * random high-entropy initial state
    * hot Hamiltonian until t_quench, then cold Hamiltonian
    * track <Z_i(t)>, lump structure, parity, occupation, etc.

CUDA note:
- If CuPy is installed and a CUDA device is available, Hamiltonian
  eigendecompositions are run on the GPU via cupy.linalg.eigh.
- Results are copied back to NumPy arrays for the rest of the pipeline.

Internal-path diagnostics:
- For each time slice, we build a coarse binary occupancy pattern:
    n_i = (1 - Z_i)/2, occupancy_i = 1 if n_i >= 0.5 else 0
- We compute Hamming distance between successive occupancy patterns.
- We flag "internal changes" where the dominant lump's center
  stays on the same site but the occupancy pattern changes.
"""

from __future__ import annotations

import argparse
import datetime as _dt
import json
import os
import sys
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Tuple, Optional

import numpy as np

# Try to enable CUDA via CuPy
try:
    import cupy as cp  # type: ignore[attr-defined]
    HAS_CUPY = True
except Exception:  # noqa: BLE001
    cp = None  # type: ignore[assignment]
    HAS_CUPY = False


# -----------------------------------------------------------------------------
# Data classes
# -----------------------------------------------------------------------------

@dataclass
class GeometryAsset:
    graph_dist: np.ndarray
    extras: Dict[str, np.ndarray]


@dataclass
class PrecipitationConfig:
    n_sites: int
    local_dim: int = 2
    J_coupling: float = 1.0
    h_field: float = 0.2
    defrag_hot: float = 0.3
    defrag_cold: float = 1.0
    t_total: float = 10.0
    t_quench: float = 4.0
    n_steps: int = 101
    z_threshold: float = 0.5


@dataclass
class GaugeConfig:
    n_sites: int
    J_coupling: float = 1.0
    h_field: float = 0.2
    defrag_zz: float = 0.0
    alpha_local_reward: float = 0.5
    w_long_base: float = 1.0
    flux_weight: float = 0.5
    n_gauge_steps: int = 30
    phase_step: float = 0.1
    fd_epsilon: float = 1e-2
    random_seed: int = 1234


@dataclass
class RunLayout:
    run_root: str
    data_dir: str
    logs_dir: str
    figures_dir: str

    params_json: str
    metadata_json: str
    summary_json: str
    timeseries_npz: str
    lump_hist_json: str
    dominant_lump_hist_json: str
    gauge_phases_json: str
    gauge_cost_history_json: str
    gauge_flux_json: str
    log_path: str


@dataclass
class LumpSnapshot:
    time: float
    n_lumps: int
    lumps: List[List[int]]


# -----------------------------------------------------------------------------
# Utilities: I/O and layout
# -----------------------------------------------------------------------------

def make_run_id(tag: str | None = None) -> str:
    ts = _dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    if tag:
        tag = tag.strip()
        if tag:
            return f"{ts}_{tag}"
    return ts


def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=False)


def write_json(path: str, data: Any) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, sort_keys=True)


def build_layout(output_root: str, tag: str | None = None) -> RunLayout:
    run_id = make_run_id(tag)
    run_root = os.path.join(output_root, "precipitating_event", run_id)

    data_dir = os.path.join(run_root, "data")
    logs_dir = os.path.join(run_root, "logs")
    figures_dir = os.path.join(run_root, "figures")

    ensure_dir(run_root)
    ensure_dir(data_dir)
    ensure_dir(logs_dir)
    ensure_dir(figures_dir)

    return RunLayout(
        run_root=run_root,
        data_dir=data_dir,
        logs_dir=logs_dir,
        figures_dir=figures_dir,
        params_json=os.path.join(run_root, "params.json"),
        metadata_json=os.path.join(run_root, "metadata.json"),
        summary_json=os.path.join(run_root, "summary.json"),
        timeseries_npz=os.path.join(data_dir, "time_series.npz"),
        lump_hist_json=os.path.join(data_dir, "lump_hist.json"),
        dominant_lump_hist_json=os.path.join(
            data_dir, "dominant_lump_hist.json"
        ),
        gauge_phases_json=os.path.join(data_dir, "gauge_phases.json"),
        gauge_cost_history_json=os.path.join(data_dir, "gauge_cost_history.json"),
        gauge_flux_json=os.path.join(data_dir, "gauge_fluxes.json"),
        log_path=os.path.join(logs_dir, "run.log"),
    )


# -----------------------------------------------------------------------------
# Geometry loading and adjacency
# -----------------------------------------------------------------------------

def load_geometry(path: str) -> GeometryAsset:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Geometry file not found: {path}")
    npz = np.load(path)
    if "graph_dist" not in npz.files:
        raise ValueError("Geometry npz must contain 'graph_dist'.")
    gd = np.array(npz["graph_dist"], dtype=float)
    extras = {k: npz[k] for k in npz.files if k != "graph_dist"}
    return GeometryAsset(graph_dist=gd, extras=extras)


def adjacency_from_graph_dist(graph_dist: np.ndarray) -> np.ndarray:
    adj = (graph_dist == 1).astype(int)
    np.fill_diagonal(adj, 0)
    return adj


def build_edge_list(adjacency: np.ndarray) -> List[Tuple[int, int]]:
    n = adjacency.shape[0]
    edges: List[Tuple[int, int]] = []
    for i in range(n):
        for j in range(i + 1, n):
            if adjacency[i, j] != 0:
                edges.append((i, j))
    return edges


# -----------------------------------------------------------------------------
# Pauli ops, Hamiltonian, evolution (NumPy side)
# -----------------------------------------------------------------------------

def pauli_matrices() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    I = np.eye(2, dtype=complex)
    X = np.array([[0, 1], [1, 0]], dtype=complex)
    Y = np.array([[0, -1j], [1j, 0]], dtype=complex)
    Z = np.array([[1, 0], [0, -1]], dtype=complex)
    return I, X, Y, Z


def kron_on_site(op: np.ndarray, site: int, n_sites: int) -> np.ndarray:
    I2 = np.eye(2, dtype=complex)
    mats = []
    for j in range(n_sites):
        mats.append(op if j == site else I2)
    out = mats[0]
    for m in mats[1:]:
        out = np.kron(out, m)
    return out


def build_local_ops(n_sites: int) -> Dict[str, List[np.ndarray]]:
    I, X, Y, Z = pauli_matrices()
    Xs: List[np.ndarray] = []
    Ys: List[np.ndarray] = []
    Zs: List[np.ndarray] = []
    for s in range(n_sites):
        Xs.append(kron_on_site(X, s, n_sites))
        Ys.append(kron_on_site(Y, s, n_sites))
        Zs.append(kron_on_site(Z, s, n_sites))
    return {"X": Xs, "Y": Ys, "Z": Zs}


def build_parity_operator(Z_ops: List[np.ndarray]) -> np.ndarray:
    P = Z_ops[0].copy()
    for Zi in Z_ops[1:]:
        P = P @ Zi
    return P


def build_hamiltonian_with_gauge(
    J: float,
    h: float,
    defrag_strength: float,
    adjacency: np.ndarray,
    local_ops: Dict[str, List[np.ndarray]],
    edges: List[Tuple[int, int]],
    edge_phases: Optional[Dict[Tuple[int, int], float]],
    use_plain_heisenberg_if_no_gauge: bool = True,
) -> np.ndarray:
    """
    H(phi) = sum_edges J * [ cos(phi) (X_i X_j + Y_i Y_j)
                             + sin(phi) (X_i Y_j - Y_i X_j)
                             + defrag_strength * Z_i Z_j ]
             - h * sum_i Z_i

    If edge_phases is None and use_plain_heisenberg_if_no_gauge=True:
      we revert to J*(XX+YY+ZZ) + defrag_strength*ZZ.

    If edge_phases is None and use_plain_heisenberg_if_no_gauge=False:
      treat all phi_ij = 0 (gauge trivial, XY only + defrag ZZ).
    """
    n_sites = adjacency.shape[0]
    Xs = local_ops["X"]
    Ys = local_ops["Y"]
    Zs = local_ops["Z"]

    dim = 2 ** n_sites
    H = np.zeros((dim, dim), dtype=complex)

    use_gauge = edge_phases is not None

    for (i, j) in edges:
        if use_gauge:
            key = (i, j) if i < j else (j, i)
            phi = float(edge_phases.get(key, 0.0))
            c = np.cos(phi)
            s = np.sin(phi)

            XX = Xs[i] @ Xs[j]
            YY = Ys[i] @ Ys[j]
            XY = Xs[i] @ Ys[j]
            YX = Ys[i] @ Xs[j]
            ZZ = Zs[i] @ Zs[j]

            H += J * (c * (XX + YY) + s * (XY - YX))
            H += defrag_strength * ZZ
        else:
            XX = Xs[i] @ Xs[j]
            YY = Ys[i] @ Ys[j]
            ZZ = Zs[i] @ Zs[j]
            if use_plain_heisenberg_if_no_gauge:
                H += J * (XX + YY + ZZ)
                H += defrag_strength * ZZ
            else:
                H += J * (XX + YY)
                H += defrag_strength * ZZ

    for i in range(n_sites):
        H += -h * Zs[i]

    H = 0.5 * (H + H.conj().T)
    return H


# -----------------------------------------------------------------------------
# CUDA-aware eigensolver
# -----------------------------------------------------------------------------

def eigh_hermitian(H: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Hermitian eigendecomposition using CuPy if available, else NumPy.

    Returns:
        evals: 1D NumPy array
        evecs: 2D NumPy array (columns = eigenvectors)
    """
    if HAS_CUPY:
        H_gpu = cp.asarray(H)  # type: ignore[call-arg]
        evals_gpu, evecs_gpu = cp.linalg.eigh(H_gpu)  # type: ignore[union-attr]
        evals = cp.asnumpy(evals_gpu)  # type: ignore[union-attr]
        evecs = cp.asnumpy(evecs_gpu)  # type: ignore[union-attr]
        return evals, evecs
    else:
        evals, evecs = np.linalg.eigh(H)
        return evals, evecs


def diagonalize(H: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    return eigh_hermitian(H)


def ground_state(H: np.ndarray) -> Tuple[float, np.ndarray]:
    evals, evecs = eigh_hermitian(H)
    idx = int(np.argmin(evals))
    E0 = float(evals[idx])
    psi0 = evecs[:, idx]
    return E0, psi0


def evolve_spectral(
    evals: np.ndarray,
    evecs: np.ndarray,
    psi0: np.ndarray,
    t: float,
) -> np.ndarray:
    alpha0 = evecs.conj().T @ psi0
    phase = np.exp(-1j * evals * t)
    alpha_t = phase * alpha0
    psi_t = evecs @ alpha_t
    return psi_t


def random_state(dim: int, rng: np.random.Generator) -> np.ndarray:
    re = rng.normal(size=dim)
    im = rng.normal(size=dim)
    v = re + 1j * im
    v /= np.linalg.norm(v)
    return v


# -----------------------------------------------------------------------------
# Correlations and gauge cost
# -----------------------------------------------------------------------------

def z_expectations(psi: np.ndarray, Z_ops: List[np.ndarray]) -> np.ndarray:
    bra = psi.conj().T
    vals = []
    for Zi in Z_ops:
        vals.append(np.real_if_close(bra @ (Zi @ psi)))
    return np.array(vals, dtype=float)


def z_corr_matrix(psi: np.ndarray, Z_ops: List[np.ndarray]) -> np.ndarray:
    n_sites = len(Z_ops)
    z_means = z_expectations(psi, Z_ops)
    C = np.zeros((n_sites, n_sites), dtype=float)
    bra = psi.conj().T
    for i in range(n_sites):
        for j in range(n_sites):
            Zij = Z_ops[i] @ Z_ops[j]
            val = np.real_if_close(bra @ (Zij @ psi))
            C[i, j] = float(val) - float(z_means[i] * z_means[j])
    return C


def entanglement_cost(
    gcfg: GaugeConfig,
    graph_dist: np.ndarray,
    C: np.ndarray,
) -> float:
    n = graph_dist.shape[0]
    cost_long = 0.0
    reward_local = 0.0

    for i in range(n):
        for j in range(i + 1, n):
            d = graph_dist[i, j]
            cij2 = float(np.abs(C[i, j]) ** 2)
            if d <= 0:
                continue
            if d == 1.0:
                reward_local += cij2
            elif d > 1.0:
                w = gcfg.w_long_base * (d ** 2)
                cost_long += w * cij2

    return cost_long - gcfg.alpha_local_reward * reward_local


def find_square_plaquettes(adjacency: np.ndarray) -> List[List[int]]:
    """
    Find simple 4-cycles (square plaquettes) in the graph.
    Returns oriented cycles [a,b,c,d].
    """
    n = adjacency.shape[0]
    squares: List[List[int]] = []
    seen: set[Tuple[int, int, int, int]] = set()

    for s in range(n):
        for i in range(n):
            if i == s or adjacency[s, i] == 0:
                continue
            for j in range(n):
                if j in (s, i) or adjacency[i, j] == 0:
                    continue
                for k in range(n):
                    if k in (s, i, j) or adjacency[j, k] == 0:
                        continue
                    if adjacency[k, s] == 0:
                        continue
                    nodes = [s, i, j, k]
                    if len(set(nodes)) != 4:
                        continue
                    key = tuple(sorted(nodes))
                    if key in seen:
                        continue
                    seen.add(key)
                    squares.append(nodes)

    return squares


def square_fluxes(
    edges: List[Tuple[int, int]],
    phases: np.ndarray,
    squares: List[List[int]],
) -> List[Dict[str, Any]]:
    """
    Flux around square [a,b,c,d] using oriented edges
    (a->b, b->c, c->d, d->a).
    """
    edge_index: Dict[Tuple[int, int], int] = {}
    for idx, (i, j) in enumerate(edges):
        if i > j:
            i, j = j, i
        edge_index[(i, j)] = idx

    out: List[Dict[str, Any]] = []
    for sq in squares:
        a, b, c, d = sq
        pairs = [(a, b), (b, c), (c, d), (d, a)]
        flux = 0.0
        legs = []
        for (u, v) in pairs:
            if u < v:
                key = (u, v)
                sign = +1.0
            else:
                key = (v, u)
                sign = -1.0
            idx = edge_index.get(key, None)
            if idx is None:
                phase_uv = 0.0
            else:
                phase_uv = sign * float(phases[idx])
            flux += phase_uv
            legs.append({"edge": [u, v], "phase_used": phase_uv})
        flux_wrapped = (flux + np.pi) % (2.0 * np.pi) - np.pi
        out.append(
            {
                "plaquette": sq,
                "flux_raw": flux,
                "flux_wrapped": flux_wrapped,
                "legs": legs,
            }
        )
    return out


def flux_cost(square_flux_list: List[Dict[str, Any]]) -> float:
    """
    Prefer ±π flux: sum_p min[(Φ_p - π)^2, (Φ_p + π)^2].
    """
    total = 0.0
    for fx in square_flux_list:
        phi = float(fx["flux_wrapped"])
        d_plus = (phi - np.pi) ** 2
        d_minus = (phi + np.pi) ** 2
        total += float(min(d_plus, d_minus))
    return total


def total_gauge_cost(
    gcfg: GaugeConfig,
    geom: GeometryAsset,
    C: np.ndarray,
    edges: List[Tuple[int, int]],
    phases: np.ndarray,
    squares: List[List[int]],
) -> Tuple[float, float, float]:
    C_ent = entanglement_cost(gcfg, geom.graph_dist, C)
    if squares:
        sq_flux = square_fluxes(edges, phases, squares)
        C_f = flux_cost(sq_flux)
    else:
        C_f = 0.0
    C_tot = C_ent + gcfg.flux_weight * C_f
    return float(C_tot), float(C_ent), float(C_f)


# -----------------------------------------------------------------------------
# Gauge optimization (inside this file)
# -----------------------------------------------------------------------------

def gauge_gradient_descent(
    geom: GeometryAsset,
    gcfg: GaugeConfig,
    adjacency: np.ndarray,
    edges: List[Tuple[int, int]],
    local_ops: Dict[str, List[np.ndarray]],
    log,
) -> Dict[str, Any]:
    n_sites = geom.graph_dist.shape[0]
    if gcfg.n_sites != n_sites:
        raise ValueError(f"GaugeConfig n_sites={gcfg.n_sites}, geometry has {n_sites}.")

    Z_ops = local_ops["Z"]
    squares = find_square_plaquettes(adjacency)

    log(f"[GAUGE] n_sites={n_sites}, n_edges={len(edges)}, n_squares={len(squares)}")
    log(f"[GAUGE] edges = {edges}")
    if squares:
        log(f"[GAUGE] squares (4-cycles) = {squares}")
    else:
        log("[GAUGE] no square plaquettes found.")
    log("-" * 60)

    rng = np.random.default_rng(gcfg.random_seed)
    phases = rng.uniform(low=-np.pi, high=np.pi, size=len(edges))

    cost_history: List[Dict[str, float]] = []

    for step in range(gcfg.n_gauge_steps):
        H = build_hamiltonian_with_gauge(
            J=gcfg.J_coupling,
            h=gcfg.h_field,
            defrag_strength=gcfg.defrag_zz,
            adjacency=adjacency,
            local_ops=local_ops,
            edges=edges,
            edge_phases={((i if i < j else j), (j if i < j else i)): float(phases[eidx])
                         for eidx, (i, j) in enumerate(edges)},
            use_plain_heisenberg_if_no_gauge=False,
        )
        E0, psi0 = ground_state(H)
        C = z_corr_matrix(psi0, Z_ops)
        C_tot, C_ent, C_flux = total_gauge_cost(
            gcfg, geom, C, edges, phases, squares
        )

        cost_history.append(
            {
                "step": float(step),
                "total": C_tot,
                "entanglement": C_ent,
                "flux": C_flux,
                "E0": E0,
            }
        )

        log(
            f"[GAUGE step {step:03d}] "
            f"E0={E0:.6f}, C_total={C_tot:.6f}, "
            f"C_ent={C_ent:.6f}, C_flux={C_flux:.6f}"
        )

        # finite-difference gradient
        grad = np.zeros_like(phases)
        eps = gcfg.fd_epsilon

        for e_idx, (i, j) in enumerate(edges):
            original = phases[e_idx]

            phases[e_idx] = original + eps
            H_plus = build_hamiltonian_with_gauge(
                J=gcfg.J_coupling,
                h=gcfg.h_field,
                defrag_strength=gcfg.defrag_zz,
                adjacency=adjacency,
                local_ops=local_ops,
                edges=edges,
                edge_phases={((ii if ii < jj else jj), (jj if ii < jj else ii)): float(phases[k])
                             for k, (ii, jj) in enumerate(edges)},
                use_plain_heisenberg_if_no_gauge=False,
            )
            E_plus, psi_plus = ground_state(H_plus)
            C_plus = z_corr_matrix(psi_plus, Z_ops)
            C_tot_plus, _, _ = total_gauge_cost(
                gcfg, geom, C_plus, edges, phases, squares
            )

            phases[e_idx] = original - eps
            H_minus = build_hamiltonian_with_gauge(
                J=gcfg.J_coupling,
                h=gcfg.h_field,
                defrag_strength=gcfg.defrag_zz,
                adjacency=adjacency,
                local_ops=local_ops,
                edges=edges,
                edge_phases={((ii if ii < jj else jj), (jj if ii < jj else ii)): float(phases[k])
                             for k, (ii, jj) in enumerate(edges)},
                use_plain_heisenberg_if_no_gauge=False,
            )
            E_minus, psi_minus = ground_state(H_minus)
            C_minus = z_corr_matrix(psi_minus, Z_ops)
            C_tot_minus, _, _ = total_gauge_cost(
                gcfg, geom, C_minus, edges, phases, squares
            )

            grad[e_idx] = (C_tot_plus - C_tot_minus) / (2.0 * eps)
            phases[e_idx] = original

        phases = phases - gcfg.phase_step * grad
        phases = (phases + np.pi) % (2.0 * np.pi) - np.pi

    # final summary
    H_final = build_hamiltonian_with_gauge(
        J=gcfg.J_coupling,
        h=gcfg.h_field,
        defrag_strength=gcfg.defrag_zz,
        adjacency=adjacency,
        local_ops=local_ops,
        edges=edges,
        edge_phases={((i if i < j else j), (j if i < j else i)): float(phases[eidx])
                     for eidx, (i, j) in enumerate(edges)},
        use_plain_heisenberg_if_no_gauge=False,
    )
    E0_final, psi_final = ground_state(H_final)
    C_final = z_corr_matrix(psi_final, Z_ops)
    C_tot_final, C_ent_final, C_flux_final = total_gauge_cost(
        gcfg, geom, C_final, edges, phases, squares
    )
    square_flux_list = square_fluxes(edges, phases, squares) if squares else []

    log("[GAUGE] optimization finished.")
    log(
        f"[GAUGE] final E0={E0_final:.6f}, "
        f"C_total={C_tot_final:.6f}, "
        f"C_ent={C_ent_final:.6f}, C_flux={C_flux_final:.6f}"
    )

    return {
        "phases": phases,
        "edges": edges,
        "cost_history": cost_history,
        "E0_final": E0_final,
        "C_final": C_final,
        "C_total_final": C_tot_final,
        "C_ent_final": C_ent_final,
        "C_flux_final": C_flux_final,
        "squares": squares,
        "square_flux_list": square_flux_list,
    }


# -----------------------------------------------------------------------------
# Lump + parity diagnostics
# -----------------------------------------------------------------------------

def local_z_expectations_full(
    psi: np.ndarray,
    Z_ops: List[np.ndarray],
) -> np.ndarray:
    return z_expectations(psi, Z_ops)


def total_occupation_from_z(z_profile: np.ndarray) -> float:
    n_i = 0.5 * (1.0 - z_profile)
    return float(np.sum(n_i))


def parity_expectation(psi: np.ndarray, P_op: np.ndarray) -> float:
    bra = psi.conj().T
    val = bra @ (P_op @ psi)
    return float(np.real_if_close(val))


def find_lumps(
    z_profile: np.ndarray,
    adjacency: np.ndarray,
    z_threshold: float,
) -> LumpSnapshot:
    n_sites = len(z_profile)
    mean_z = float(np.mean(z_profile))
    active = np.abs(z_profile - mean_z) >= z_threshold

    visited = np.zeros(n_sites, dtype=bool)
    lumps: List[List[int]] = []

    for i in range(n_sites):
        if not active[i] or visited[i]:
            continue
        comp = [i]
        visited[i] = True
        stack = [i]
        while stack:
            u = stack.pop()
            for v in range(n_sites):
                if adjacency[u, v] != 0 and active[v] and not visited[v]:
                    visited[v] = True
                    comp.append(v)
                    stack.append(v)
        lumps.append(sorted(comp))

    return LumpSnapshot(time=0.0, n_lumps=len(lumps), lumps=lumps)


def extract_dominant_lump_timeline(
    times: np.ndarray,
    lump_hist: List[List[List[int]]],
) -> Dict[str, Any]:
    n_steps = len(times)
    dominant_sizes: List[int] = []
    dominant_sites: List[List[int]] = []

    for t_idx in range(n_steps):
        lumps_t = lump_hist[t_idx]
        if not lumps_t:
            dominant_sizes.append(0)
            dominant_sites.append([])
            continue
        best = max(lumps_t, key=lambda L: len(L))
        dominant_sizes.append(len(best))
        dominant_sites.append(best)

    sizes_arr = np.array(dominant_sizes, dtype=int)
    has_any = sizes_arr > 0
    n_any = int(np.sum(has_any))
    frac_any = float(n_any) / float(n_steps) if n_steps > 0 else 0.0
    max_size = int(np.max(sizes_arr)) if n_steps > 0 else 0
    mean_size = float(np.mean(sizes_arr)) if n_steps > 0 else 0.0

    if n_any > 0:
        first_idx = int(np.argmax(has_any))
        last_idx = int(len(has_any) - 1 - np.argmax(has_any[::-1]))
        first_time = float(times[first_idx])
        last_time = float(times[last_idx])
    else:
        first_idx = -1
        last_idx = -1
        first_time = None
        last_time = None

    metrics = {
        "timesteps_with_any_lump": n_any,
        "fraction_time_with_any_lump": frac_any,
        "max_dominant_lump_size": max_size,
        "mean_dominant_lump_size": mean_size,
        "first_lump_t_index": first_idx,
        "last_lump_t_index": last_idx,
        "first_lump_time": first_time,
        "last_lump_time": last_time,
    }

    return {
        "dominant_sizes": dominant_sizes,
        "dominant_sites": dominant_sites,
        "metrics": metrics,
    }


# -----------------------------------------------------------------------------
# Precipitating Event driver (with internal-path diagnostics)
# -----------------------------------------------------------------------------

def run_precipitating_event(
    geom: GeometryAsset,
    cfg: PrecipitationConfig,
    seed: int,
    adjacency: np.ndarray,
    edges: List[Tuple[int, int]],
    local_ops: Dict[str, List[np.ndarray]],
    edge_phases: Optional[Dict[Tuple[int, int], float]],
) -> Dict[str, Any]:
    rng = np.random.default_rng(seed)
    n_sites = cfg.n_sites

    Z_ops = local_ops["Z"]
    P_op = build_parity_operator(Z_ops)

    H_hot = build_hamiltonian_with_gauge(
        J=cfg.J_coupling,
        h=cfg.h_field,
        defrag_strength=cfg.defrag_hot,
        adjacency=adjacency,
        local_ops=local_ops,
        edges=edges,
        edge_phases=edge_phases,
        use_plain_heisenberg_if_no_gauge=True,
    )
    H_cold = build_hamiltonian_with_gauge(
        J=cfg.J_coupling,
        h=cfg.h_field,
        defrag_strength=cfg.defrag_cold,
        adjacency=adjacency,
        local_ops=local_ops,
        edges=edges,
        edge_phases=edge_phases,
        use_plain_heisenberg_if_no_gauge=True,
    )

    evals_hot, evecs_hot = diagonalize(H_hot)
    evals_cold, evecs_cold = diagonalize(H_cold)

    times = np.linspace(0.0, cfg.t_total, cfg.n_steps)
    if not (0.0 < cfg.t_quench < cfg.t_total):
        raise ValueError("Require 0 < t_quench < t_total.")
    k_quench = int(np.argmin(np.abs(times - cfg.t_quench)))
    t_quench_eff = float(times[k_quench])

    dim = cfg.local_dim ** cfg.n_sites
    psi0 = random_state(dim, rng)
    psi_quench = evolve_spectral(evals_hot, evecs_hot, psi0, t_quench_eff)

    local_z_t = np.zeros((cfg.n_steps, n_sites), dtype=float)
    lump_counts = np.zeros(cfg.n_steps, dtype=int)
    lump_hist: List[List[List[int]]] = []
    parity_t = np.zeros(cfg.n_steps, dtype=float)
    total_N_t = np.zeros(cfg.n_steps, dtype=float)

    # Internal-path diagnostics
    occupancy_t = np.zeros((cfg.n_steps, n_sites), dtype=int)
    hamming_t = np.zeros(cfg.n_steps, dtype=int)
    # internal_change_t computed after we know dominant centers

    for idx, t in enumerate(times):
        if t <= t_quench_eff + 1e-12:
            psi_t = evolve_spectral(evals_hot, evecs_hot, psi0, t)
        else:
            dt = float(t - t_quench_eff)
            psi_t = evolve_spectral(evals_cold, evecs_cold, psi_quench, dt)

        z_profile = local_z_expectations_full(psi_t, Z_ops)
        local_z_t[idx, :] = z_profile
        total_N_t[idx] = total_occupation_from_z(z_profile)
        parity_t[idx] = parity_expectation(psi_t, P_op)

        # Find lumps in Z profile
        snap = find_lumps(
            z_profile=z_profile,
            adjacency=adjacency,
            z_threshold=cfg.z_threshold,
        )
        lump_counts[idx] = snap.n_lumps
        lump_hist.append(snap.lumps)

        # Coarse occupancy pattern from Z:
        # n_i = (1 - Z_i)/2, occupancy_i = 1 if n_i >= 0.5 else 0
        n_i = 0.5 * (1.0 - z_profile)
        occ = (n_i >= 0.5).astype(int)
        occupancy_t[idx, :] = occ
        if idx > 0:
            # Hamming distance to previous pattern
            hamming_t[idx] = int(np.sum(np.bitwise_xor(occupancy_t[idx - 1, :], occ)))

    final_z = local_z_t[-1, :]
    final_snap = find_lumps(
        z_profile=final_z,
        adjacency=adjacency,
        z_threshold=cfg.z_threshold,
    )
    final_lump_sizes = [len(L) for L in final_snap.lumps]

    metrics_basic: Dict[str, Any] = {
        "t_quench_index": int(k_quench),
        "t_quench_effective": t_quench_eff,
        "final_n_lumps": int(final_snap.n_lumps),
        "final_lump_sizes": final_lump_sizes,
        "mean_lump_count": float(np.mean(lump_counts)),
        "has_particle_candidates": bool(final_snap.n_lumps > 0),
    }

    dom_info = extract_dominant_lump_timeline(times, lump_hist)
    dominant_metrics = dom_info["metrics"]

    # Internal-change detection:
    # We define a "center" as the first site of the dominant lump (if any).
    centers = np.full(cfg.n_steps, -1, dtype=int)
    for t_idx, sites in enumerate(dom_info["dominant_sites"]):
        if sites:
            centers[t_idx] = int(sites[0])

    internal_change_t = np.zeros(cfg.n_steps, dtype=bool)
    for t_idx in range(1, cfg.n_steps):
        if centers[t_idx] != -1 and centers[t_idx] == centers[t_idx - 1]:
            if hamming_t[t_idx] > 0:
                internal_change_t[t_idx] = True

    internal_change_count = int(np.sum(internal_change_t))
    denom = max(cfg.n_steps - 1, 1)
    internal_change_fraction = float(internal_change_count) / float(denom)

    parity_mean = float(np.mean(parity_t))
    parity_std = float(np.std(parity_t))
    N_mean = float(np.mean(total_N_t))
    N_std = float(np.std(total_N_t))

    metrics = {
        **metrics_basic,
        **{
            "timesteps_with_any_lump": dominant_metrics["timesteps_with_any_lump"],
            "fraction_time_with_any_lump": dominant_metrics["fraction_time_with_any_lump"],
            "max_dominant_lump_size": dominant_metrics["max_dominant_lump_size"],
            "mean_dominant_lump_size": dominant_metrics["mean_dominant_lump_size"],
            "first_lump_t_index": dominant_metrics["first_lump_t_index"],
            "last_lump_t_index": dominant_metrics["last_lump_t_index"],
            "first_lump_time": dominant_metrics["first_lump_time"],
            "last_lump_time": dominant_metrics["last_lump_time"],
            "parity_mean": parity_mean,
            "parity_std": parity_std,
            "N_mean": N_mean,
            "N_std": N_std,
            "uses_gauge_phases": bool(edge_phases is not None),
            "internal_change_count": internal_change_count,
            "internal_change_fraction": internal_change_fraction,
            "has_internal_dynamics": bool(internal_change_count > 0),
        },
    }

    return {
        "config": asdict(cfg),
        "times": times,
        "local_z_t": local_z_t,
        "lump_counts": lump_counts,
        "lump_hist": lump_hist,
        "dominant_lump_sizes": dom_info["dominant_sizes"],
        "dominant_lump_sites": dom_info["dominant_sites"],
        "parity_t": parity_t,
        "total_N_t": total_N_t,
        # internal-path diagnostics:
        "occupancy_t": occupancy_t,
        "hamming_t": hamming_t,
        "internal_change_t": internal_change_t.tolist(),
        "metrics": metrics,
    }


# -----------------------------------------------------------------------------
# CLI
# -----------------------------------------------------------------------------

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description=(
            "Hilbert-substrate Precipitating Event on emergent geometry "
            "(fermion-ready, with optional internal gauge optimization, CUDA-aware)."
        )
    )

    # Paths / run organisation
    p.add_argument(
        "--geometry",
        type=str,
        default="lr_embedding_3d.npz",
        help="Path to emergent geometry npz (must contain graph_dist).",
    )
    p.add_argument(
        "--output-root",
        type=str,
        default="outputs",
        help="Root directory to hold all runs.",
    )
    p.add_argument(
        "--tag",
        type=str,
        default="",
        help="Optional tag appended to run_id.",
    )
    p.add_argument(
        "--seed",
        type=int,
        default=117,
        help="Random seed used for both gauge and event.",
    )

    # Precipitating Event parameters
    p.add_argument("--t-total", type=float, default=10.0)
    p.add_argument("--t-quench", type=float, default=4.0)
    p.add_argument("--n-steps", type=int, default=101)
    p.add_argument("--J-coupling", type=float, default=1.0)
    p.add_argument("--h-field", type=float, default=0.2)
    p.add_argument("--defrag-hot", type=float, default=0.3)
    p.add_argument("--defrag-cold", type=float, default=1.0)
    p.add_argument("--z-threshold", type=float, default=0.5)

    # Gauge control
    p.add_argument(
        "--use-gauge",
        action="store_true",
        help="If set, run internal gauge optimization before the event.",
    )
    p.add_argument(
        "--gauge-flux-weight",
        type=float,
        default=0.5,
        help="Weight for square-plaquette flux term (prefers ±π).",
    )
    p.add_argument(
        "--gauge-alpha-local-reward",
        type=float,
        default=0.5,
        help="Weight for rewarding nearest-neighbor correlations.",
    )
    p.add_argument(
        "--gauge-w-long-base",
        type=float,
        default=1.0,
        help="Base weight for long-range correlation penalty.",
    )
    p.add_argument(
        "--gauge-n-steps",
        type=int,
        default=30,
        help="Number of gradient steps for gauge optimization.",
    )
    p.add_argument(
        "--gauge-phase-step",
        type=float,
        default=0.1,
        help="Gradient step size for gauge phases.",
    )
    p.add_argument(
        "--gauge-fd-epsilon",
        type=float,
        default=1e-2,
        help="Finite-difference epsilon for gauge gradients.",
    )
    p.add_argument(
        "--gauge-defrag-zz",
        type=float,
        default=0.0,
        help="ZZ strength used in gauge Hamiltonian (independent of event defrag).",
    )

    return p.parse_args()


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

def main() -> None:
    args = parse_args()

    # Layout + logging
    try:
        layout = build_layout(output_root=args.output_root, tag=args.tag)
    except FileExistsError:
        print(
            f"[ERROR] Run directory already exists. "
            f"Try a different --tag or wait 1s.\n"
            f"output_root={args.output_root}",
            file=sys.stderr,
        )
        sys.exit(1)

    log_f = open(layout.log_path, "w", encoding="utf-8")

    def log(msg: str) -> None:
        print(msg)
        print(msg, file=log_f, flush=True)

    log("=" * 60)
    log("  Hilbert-Substrate Precipitating Event (single-file, gauge-ready, CUDA-aware)")
    log("=" * 60)
    log(f"Run root:      {layout.run_root}")
    log(f"Geometry file: {args.geometry}")
    log(f"Seed:          {args.seed}")
    log(f"Use gauge:     {args.use_gauge}")
    log(f"CuPy/CUDA:     {'ENABLED' if HAS_CUPY else 'DISABLED'}")
    log("-" * 60)

    # Load geometry
    try:
        geom = load_geometry(args.geometry)
    except Exception as e:  # noqa: BLE001
        log(f"[ERROR] Failed to load geometry: {e}")
        log_f.close()
        sys.exit(1)

    n_sites = geom.graph_dist.shape[0]
    adjacency = adjacency_from_graph_dist(geom.graph_dist)
    edges = build_edge_list(adjacency)
    local_ops = build_local_ops(n_sites)

    log(f"Geometry n_sites: {n_sites}")
    log(f"Edges:            {edges}")
    log("-" * 60)

    # Precipitation config
    pcfg = PrecipitationConfig(
        n_sites=n_sites,
        local_dim=2,
        J_coupling=float(args.J_coupling),
        h_field=float(args.h_field),
        defrag_hot=float(args.defrag_hot),
        defrag_cold=float(args.defrag_cold),
        t_total=float(args.t_total),
        t_quench=float(args.t_quench),
        n_steps=int(args.n_steps),
        z_threshold=float(args.z_threshold),
    )

    # Gauge config (if used)
    if args.use_gauge:
        gcfg = GaugeConfig(
            n_sites=n_sites,
            J_coupling=float(args.J_coupling),
            h_field=float(args.h_field),
            defrag_zz=float(args.gauge_defrag_zz),
            alpha_local_reward=float(args.gauge_alpha_local_reward),
            w_long_base=float(args.gauge_w_long_base),
            flux_weight=float(args.gauge_flux_weight),
            n_gauge_steps=int(args.gauge_n_steps),
            phase_step=float(args.gauge_phase_step),
            fd_epsilon=float(args.gauge_fd_epsilon),
            random_seed=int(args.seed),
        )
    else:
        gcfg = None  # type: ignore

    # Save params
    params_payload = {
        "timestamp": _dt.datetime.now().isoformat(),
        "precipitation_config": asdict(pcfg),
        "gauge_config": asdict(gcfg) if gcfg is not None else None,
    }
    write_json(layout.params_json, params_payload)

    metadata_payload = {
        "script": os.path.basename(__file__),
        "run_root": os.path.abspath(layout.run_root),
        "geometry_file": os.path.abspath(args.geometry),
        "seed": int(args.seed),
        "args": vars(args),
        "extras_in_geometry": list(geom.extras.keys()),
    }
    write_json(layout.metadata_json, metadata_payload)

    # -------------------------------
    # Gauge stage (optional)
    # -------------------------------
    edge_phases_dict: Optional[Dict[Tuple[int, int], float]] = None
    gauge_summary: Dict[str, Any] = {}

    if args.use_gauge:
        log("[GAUGE] Starting internal gauge optimization...")
        result = gauge_gradient_descent(
            geom=geom,
            gcfg=gcfg,  # type: ignore[arg-type]
            adjacency=adjacency,
            edges=edges,
            local_ops=local_ops,
            log=log,
        )
        phases = result["phases"]
        cost_history = result["cost_history"]
        square_flux_list = result["square_flux_list"]

        # Save gauge artifacts
        write_json(
            layout.gauge_phases_json,
            {
                "edges": edges,
                "phases": phases.tolist(),
            },
        )
        write_json(layout.gauge_cost_history_json, cost_history)
        write_json(
            layout.gauge_flux_json,
            {
                "squares": result["squares"],
                "square_fluxes": square_flux_list,
            },
        )

        edge_phases_dict = {}
        for eidx, (i, j) in enumerate(edges):
            key = (i, j) if i < j else (j, i)
            edge_phases_dict[key] = float(phases[eidx])

        gauge_summary = {
            "E0_final": float(result["E0_final"]),
            "cost_total_final": float(result["C_total_final"]),
            "cost_ent_final": float(result["C_ent_final"]),
            "cost_flux_final": float(result["C_flux_final"]),
            "n_gauge_steps": len(cost_history),
            "square_fluxes": square_flux_list,
        }
        log("[GAUGE] Phase extraction complete.")
        log("-" * 60)
    else:
        log("[GAUGE] Skipped (use plain Heisenberg/defrag).")
        log("-" * 60)

    # -------------------------------
    # Precipitating Event
    # -------------------------------
    log("Configuration (Precipitating Event):")
    for k, v in asdict(pcfg).items():
        log(f"  {k:16s} = {v}")
    log("-" * 60)
    log("Starting Precipitating Event evolution...")
    log("-" * 60)

    try:
        results = run_precipitating_event(
            geom=geom,
            cfg=pcfg,
            seed=int(args.seed),
            adjacency=adjacency,
            edges=edges,
            local_ops=local_ops,
            edge_phases=edge_phases_dict,
        )
    except Exception as e:  # noqa: BLE001
        log(f"[ERROR] Evolution failed: {e}")
        log_f.close()
        sys.exit(1)

    log("Evolution complete.")
    log("-" * 60)

    # Save time series / lumps / internal diagnostics
    np.savez_compressed(
        layout.timeseries_npz,
        times=results["times"],
        local_z_t=results["local_z_t"],
        lump_counts=results["lump_counts"],
        dominant_lump_sizes=np.array(results["dominant_lump_sizes"], dtype=int),
        parity_t=results["parity_t"],
        total_N_t=results["total_N_t"],
        occupancy_t=results["occupancy_t"],
        hamming_t=results["hamming_t"],
        internal_change_t=np.array(results["internal_change_t"], dtype=bool),
    )
    write_json(layout.lump_hist_json, results["lump_hist"])
    write_json(layout.dominant_lump_hist_json, results["dominant_lump_sites"])

    # Build and save summary
    metrics = results["metrics"]
    summary_payload = {
        "timestamp": _dt.datetime.now().isoformat(),
        "precipitation_config": results["config"],
        "metrics": metrics,
        "gauge_summary": gauge_summary if args.use_gauge else None,
    }
    write_json(layout.summary_json, summary_payload)

    # Log summary
    log("==== Precipitating Event Summary ====")
    log(f"t_quench_effective:      {metrics['t_quench_effective']:.6f}")
    log(f"final_n_lumps:           {metrics['final_n_lumps']}")
    log(f"final_lump_sizes:        {metrics['final_lump_sizes']}")
    log(f"mean_lump_count:         {metrics['mean_lump_count']:.3f}")
    log(f"has_candidates:          {metrics['has_particle_candidates']}")
    log(f"timesteps_with_any_lump: {metrics['timesteps_with_any_lump']}")
    log(f"fraction_time_with_any:  {metrics['fraction_time_with_any_lump']:.3f}")
    log(f"max_dominant_size:       {metrics['max_dominant_lump_size']}")
    log(f"mean_dominant_size:      {metrics['mean_dominant_lump_size']:.3f}")
    log(f"first_lump_time:         {metrics['first_lump_time']}")
    log(f"last_lump_time:          {metrics['last_lump_time']}")
    log(f"parity_mean:             {metrics['parity_mean']:.6f}")
    log(f"parity_std:              {metrics['parity_std']:.6f}")
    log(f"N_mean:                  {metrics['N_mean']:.6f}")
    log(f"N_std:                   {metrics['N_std']:.6f}")
    log(f"uses_gauge_phases:       {metrics['uses_gauge_phases']}")
    log(f"internal_change_count:   {metrics['internal_change_count']}")
    log(f"internal_change_frac:    {metrics['internal_change_fraction']:.3f}")
    if args.use_gauge:
        log("-" * 60)
        log("Gauge summary:")
        log(f"  final gauge E0:        {gauge_summary['E0_final']:.6f}")
        log(f"  final gauge C_total:   {gauge_summary['cost_total_final']:.6f}")
        log(f"  final gauge C_ent:     {gauge_summary['cost_ent_final']:.6f}")
        log(f"  final gauge C_flux:    {gauge_summary['cost_flux_final']:.6f}")
    log("======================================")

    log_f.close()


if __name__ == "__main__":
    main()
